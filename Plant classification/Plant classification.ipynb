{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "339bf761",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.kaggle.com/c/plant-seedlings-classification/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907c1f9",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcc7558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: # For inline backend\n"
     ]
    }
   ],
   "source": [
    "# Importing frequently used libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import tensorflow as tf  # For machine learning and neural networks\n",
    "import pickle  # For serializing and de-serializing Python object structures\n",
    "from tensorflow import keras  # For neural networks\n",
    "from tensorflow.keras.models import Sequential, Model, load_model  # For sequential and functional API models\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout, InputLayer  # For different types of layers in a neural network\n",
    "from tensorflow.keras.optimizers import Adam  # For the Adam optimizer\n",
    "from tensorflow.keras.metrics import categorical_crossentropy, MeanSquaredError  # For different types of loss functions\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For real-time data augmentation\n",
    "from tensorflow.keras.utils import to_categorical  # For one-hot encoding\n",
    "from tensorflow.keras.datasets import cifar100  # CIFAR-100 dataset\n",
    "from sklearn.metrics import confusion_matrix  # For creating a confusion matrix\n",
    "from sklearn.model_selection import train_test_split  # For splitting the data into train and test sets\n",
    "from sklearn.model_selection import KFold  # For K-Fold cross validation\n",
    "import itertools  # For creating iterators for efficient looping\n",
    "import os  # For interacting with the OS\n",
    "import shutil  # For high-level file operations\n",
    "import torch  # PyTorch library for machine learning\n",
    "import random  # For generating random numbers\n",
    "import glob  # For finding all the pathnames matching a specified pattern\n",
    "import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations in Python\n",
    "import warnings  # For warning control\n",
    "import tensorflow.keras.applications.efficientnet as efn  # For EfficientNet model\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)  # Ignore future warnings\n",
    "%matplotlib inline  # For inline backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823cf45b",
   "metadata": {},
   "source": [
    "## Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65045da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for the datasets\n",
    "base_dir = r'C:\\Users\\RezaHardMan\\Documents\\Python projects\\datasets\\plain classification'\n",
    "\n",
    "# Path to the training data\n",
    "train_path = os.path.join(base_dir, 'train')  # Joining the base directory with the specific folder name\n",
    "\n",
    "# Path to the validation data\n",
    "valid_path = os.path.join(base_dir, 'valid')  # Joining the base directory with the specific folder name\n",
    "\n",
    "# Path to the test data\n",
    "test_path = os.path.join(base_dir, 'test')  # Joining the base directory with the specific folder name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785bd5e",
   "metadata": {},
   "source": [
    "## Organize data into train, valid, test dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae099d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of directories\n",
    "list1 = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n",
    "         'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n",
    "\n",
    "# Base directory for the datasets\n",
    "base_dir = 'C:/Users/RezaHardMan/Documents/Python projects/datasets/plain classification/'\n",
    "\n",
    "# Loop over the directories\n",
    "for dir_name in list1:\n",
    "    # Construct the path to the directory\n",
    "    dir_path = os.path.join(base_dir, 'valid', dir_name)\n",
    "    \n",
    "    # If the directory does not exist, create it\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        \n",
    "    # Construct the path to the training images for this directory\n",
    "    train_path = os.path.join(base_dir, 'train', dir_name, '*.png')\n",
    "    \n",
    "    # Move a random sample of 50 images from the training set to the validation set\n",
    "    for img_path in random.sample(glob.glob(train_path), 50):\n",
    "        shutil.move(img_path, dir_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27c6be",
   "metadata": {},
   "source": [
    "## Generating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0e4292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 12 classes.\n",
      "Found 1200 images belonging to 12 classes.\n",
      "Found 794 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "# List of classes\n",
    "classes = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n",
    "           'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n",
    "\n",
    "# Creating an ImageDataGenerator for the training set with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    shear_range=0.15,  # set range for random shear\n",
    "    zoom_range=0.1,  # set range for random zoom\n",
    "    channel_shift_range=10.,  # set range for random channel shifts\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input  # preprocess input in the same manner as VGG16 model\n",
    ")\n",
    "\n",
    "# Using the above ImageDataGenerator to read images from the directory and feed to the model\n",
    "train_batches = train_datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(224,224),  # resize the images to (224, 224)\n",
    "    classes=classes,  # specify the classes\n",
    "    batch_size=10  # number of images to process at a time\n",
    ")\n",
    "\n",
    "# Creating an ImageDataGenerator for the validation set without data augmentation\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n",
    "\n",
    "# Using the above ImageDataGenerator to read images from the directory and feed to the model\n",
    "valid_batches = valid_datagen.flow_from_directory(\n",
    "    directory=valid_path,\n",
    "    target_size=(224,224),\n",
    "    classes=classes,\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "# Creating an ImageDataGenerator for the test set without data augmentation\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n",
    "\n",
    "# Using the above ImageDataGenerator to read images from the directory and feed to the model\n",
    "test_batches = test_datagen.flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=(224,224),\n",
    "    classes=['Black-grass'],  # specify the classes\n",
    "    batch_size=10,\n",
    "    shuffle=False  # do not shuffle the data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d63bb7",
   "metadata": {},
   "source": [
    "## Visualising a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5ab7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images_arr):\n",
    "    \"\"\"\n",
    "    Function to plot a list of images.\n",
    "\n",
    "    Parameters:\n",
    "    images_arr : list\n",
    "        List of images to be plotted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a figure and a set of subplots\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n",
    "    \n",
    "    # Flatten the array of Axes instances into a one-dimensional array\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over the images and corresponding axes\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        # Display an image on the axes\n",
    "        ax.imshow(img)\n",
    "        \n",
    "        # Hide the axes\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Adjust the padding between and around the subplots\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the figure\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca298c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The labels for the above images are (in one-hot encoded format):\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Get the next batch of images and labels from the training set\n",
    "imgs, labels = next(train_batches)\n",
    "\n",
    "# Plot the images using the previously defined function\n",
    "plotImages(imgs)\n",
    "\n",
    "# Print the labels corresponding to the images\n",
    "# These labels are in one-hot encoded format\n",
    "print(\"The labels for the above images are (in one-hot encoded format):\")\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9756371",
   "metadata": {},
   "source": [
    "## Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37db5de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 16)        1216      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 16)        2320      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 16, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 4, 4, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 2, 2, 256)         295168    \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 2, 2, 256)         590080    \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 2, 2, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 1, 1, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               51300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,137,972\n",
      "Trainable params: 2,137,492\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Sequential model\n",
    "model = Sequential([\n",
    "    # Add a Convolutional layer with 16 filters of size 5x5, ReLU activation function, \n",
    "    # 'same' padding (output size = input size) and input shape of 32x32 with 3 channels\n",
    "    Conv2D(filters=16, kernel_size=(5, 5), activation='relu', padding = 'same', input_shape=(32,32,3)),\n",
    "    \n",
    "    # Add another Convolutional layer with 16 filters of size 3x3 and ReLU activation function\n",
    "    Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    \n",
    "    # Add a Batch Normalization layer to normalize the activations of the previous layer\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Add a Max Pooling layer with pool size of 2x2 and strides of 2 to reduce the spatial dimensions\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    \n",
    "    # Repeat the same structure of layers but with increasing number of filters in Convolutional layers\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),    \n",
    "    Dropout(0.2),  # Add a Dropout layer after Max Pooling to prevent overfitting\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    \n",
    "    # Flatten the tensor output by the previous layer\n",
    "    Flatten(),\n",
    "    \n",
    "    # Add a Dense layer with 512 units and ReLU activation function\n",
    "    Dense(512, activation = 'relu'),\n",
    "    \n",
    "    Dropout(0.5),  # Add a Dropout layer after Dense layer to prevent overfitting\n",
    "\n",
    "    # Add a Dense output layer with 100 units (for 100 classes) and softmax activation function\n",
    "    Dense(units=100, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "print(model.summary())\n",
    "\n",
    "# Compile the model with Adam optimizer (with learning rate of 0.0001), \n",
    "# categorical crossentropy as the loss function (suitable for multi-class classification) \n",
    "# and accuracy as the metric\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "# Define the filepaths for saving the weights of the model after each epoch\n",
    "# The weights are saved in HDF5 format\n",
    "filepath1=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filepath2 = \"best_weights.hdf5\"\n",
    "\n",
    "# Define ModelCheckpoint callbacks to save the weights\n",
    "# The weights are saved only when the validation accuracy improves\n",
    "checkpoint1 = ModelCheckpoint(filepath1, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint2 = ModelCheckpoint(filepath2, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#  List of callbacks to apply during training\n",
    "callbacks_list = [checkpoint1,checkpoint2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d401b",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "800bd23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 1.8471 - accuracy: 0.3817\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.29667, saving model to weights-improvement-01-0.30.hdf5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.29667, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 71s 171ms/step - loss: 1.8471 - accuracy: 0.3817 - val_loss: 1.9002 - val_accuracy: 0.2967\n",
      "Epoch 2/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 1.3836 - accuracy: 0.5229\n",
      "Epoch 00002: val_accuracy improved from 0.29667 to 0.62500, saving model to weights-improvement-02-0.62.hdf5\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.29667 to 0.62500, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 62s 150ms/step - loss: 1.3836 - accuracy: 0.5229 - val_loss: 1.0494 - val_accuracy: 0.6250\n",
      "Epoch 3/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 1.1211 - accuracy: 0.6253\n",
      "Epoch 00003: val_accuracy did not improve from 0.62500\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.62500\n",
      "415/415 [==============================] - 62s 148ms/step - loss: 1.1211 - accuracy: 0.6253 - val_loss: 1.7216 - val_accuracy: 0.5167\n",
      "Epoch 4/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.9594 - accuracy: 0.6882\n",
      "Epoch 00004: val_accuracy improved from 0.62500 to 0.74333, saving model to weights-improvement-04-0.74.hdf5\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.62500 to 0.74333, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 63s 151ms/step - loss: 0.9594 - accuracy: 0.6882 - val_loss: 0.7394 - val_accuracy: 0.7433\n",
      "Epoch 5/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.8397 - accuracy: 0.7101\n",
      "Epoch 00005: val_accuracy did not improve from 0.74333\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.74333\n",
      "415/415 [==============================] - 62s 149ms/step - loss: 0.8397 - accuracy: 0.7101 - val_loss: 0.8218 - val_accuracy: 0.6833\n",
      "Epoch 6/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.7177 - accuracy: 0.7523\n",
      "Epoch 00006: val_accuracy improved from 0.74333 to 0.78167, saving model to weights-improvement-06-0.78.hdf5\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.74333 to 0.78167, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 62s 150ms/step - loss: 0.7177 - accuracy: 0.7523 - val_loss: 0.6197 - val_accuracy: 0.7817\n",
      "Epoch 7/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.6418 - accuracy: 0.7812\n",
      "Epoch 00007: val_accuracy improved from 0.78167 to 0.79000, saving model to weights-improvement-07-0.79.hdf5\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.78167 to 0.79000, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 62s 149ms/step - loss: 0.6418 - accuracy: 0.7812 - val_loss: 0.5110 - val_accuracy: 0.7900\n",
      "Epoch 8/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.5578 - accuracy: 0.8082\n",
      "Epoch 00008: val_accuracy did not improve from 0.79000\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.79000\n",
      "415/415 [==============================] - 62s 150ms/step - loss: 0.5578 - accuracy: 0.8082 - val_loss: 0.5785 - val_accuracy: 0.7783\n",
      "Epoch 9/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.8265\n",
      "Epoch 00009: val_accuracy improved from 0.79000 to 0.81333, saving model to weights-improvement-09-0.81.hdf5\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.79000 to 0.81333, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 62s 150ms/step - loss: 0.5037 - accuracy: 0.8265 - val_loss: 0.5339 - val_accuracy: 0.8133\n",
      "Epoch 10/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.4841 - accuracy: 0.8357\n",
      "Epoch 00010: val_accuracy improved from 0.81333 to 0.84833, saving model to weights-improvement-10-0.85.hdf5\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.81333 to 0.84833, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 61s 148ms/step - loss: 0.4841 - accuracy: 0.8357 - val_loss: 0.3956 - val_accuracy: 0.8483\n",
      "Epoch 11/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.4671 - accuracy: 0.8439\n",
      "Epoch 00011: val_accuracy did not improve from 0.84833\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.84833\n",
      "415/415 [==============================] - 60s 143ms/step - loss: 0.4671 - accuracy: 0.8439 - val_loss: 0.4783 - val_accuracy: 0.8250\n",
      "Epoch 12/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.8610\n",
      "Epoch 00012: val_accuracy did not improve from 0.84833\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.84833\n",
      "415/415 [==============================] - 61s 146ms/step - loss: 0.4154 - accuracy: 0.8610 - val_loss: 0.5740 - val_accuracy: 0.8000\n",
      "Epoch 13/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.8667\n",
      "Epoch 00013: val_accuracy improved from 0.84833 to 0.87333, saving model to weights-improvement-13-0.87.hdf5\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.84833 to 0.87333, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 60s 145ms/step - loss: 0.3883 - accuracy: 0.8667 - val_loss: 0.3355 - val_accuracy: 0.8733\n",
      "Epoch 14/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.8708\n",
      "Epoch 00014: val_accuracy improved from 0.87333 to 0.88500, saving model to weights-improvement-14-0.88.hdf5\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.87333 to 0.88500, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 60s 144ms/step - loss: 0.3676 - accuracy: 0.8708 - val_loss: 0.2832 - val_accuracy: 0.8850\n",
      "Epoch 15/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.8831\n",
      "Epoch 00015: val_accuracy did not improve from 0.88500\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.88500\n",
      "415/415 [==============================] - 60s 143ms/step - loss: 0.3381 - accuracy: 0.8831 - val_loss: 0.4937 - val_accuracy: 0.8033\n",
      "Epoch 16/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.8920\n",
      "Epoch 00016: val_accuracy did not improve from 0.88500\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.88500\n",
      "415/415 [==============================] - 60s 146ms/step - loss: 0.3345 - accuracy: 0.8920 - val_loss: 0.3182 - val_accuracy: 0.8683\n",
      "Epoch 17/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.8901\n",
      "Epoch 00017: val_accuracy did not improve from 0.88500\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.88500\n",
      "415/415 [==============================] - 61s 146ms/step - loss: 0.3093 - accuracy: 0.8901 - val_loss: 0.3384 - val_accuracy: 0.8767\n",
      "Epoch 18/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.9010\n",
      "Epoch 00018: val_accuracy did not improve from 0.88500\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.88500\n",
      "415/415 [==============================] - 60s 144ms/step - loss: 0.2815 - accuracy: 0.9010 - val_loss: 0.4345 - val_accuracy: 0.8450\n",
      "Epoch 19/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2826 - accuracy: 0.8998\n",
      "Epoch 00019: val_accuracy did not improve from 0.88500\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.88500\n",
      "415/415 [==============================] - 60s 144ms/step - loss: 0.2826 - accuracy: 0.8998 - val_loss: 0.3316 - val_accuracy: 0.8817\n",
      "Epoch 20/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.9017\n",
      "Epoch 00020: val_accuracy improved from 0.88500 to 0.89667, saving model to weights-improvement-20-0.90.hdf5\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.88500 to 0.89667, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 61s 146ms/step - loss: 0.2803 - accuracy: 0.9017 - val_loss: 0.2794 - val_accuracy: 0.8967\n",
      "Epoch 21/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.9111\n",
      "Epoch 00021: val_accuracy improved from 0.89667 to 0.90167, saving model to weights-improvement-21-0.90.hdf5\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.89667 to 0.90167, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 60s 145ms/step - loss: 0.2557 - accuracy: 0.9111 - val_loss: 0.2849 - val_accuracy: 0.9017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.9125\n",
      "Epoch 00022: val_accuracy did not improve from 0.90167\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.90167\n",
      "415/415 [==============================] - 60s 144ms/step - loss: 0.2604 - accuracy: 0.9125 - val_loss: 0.4874 - val_accuracy: 0.8367\n",
      "Epoch 23/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2232 - accuracy: 0.9166\n",
      "Epoch 00023: val_accuracy did not improve from 0.90167\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.90167\n",
      "415/415 [==============================] - 60s 143ms/step - loss: 0.2232 - accuracy: 0.9166 - val_loss: 0.4536 - val_accuracy: 0.8500\n",
      "Epoch 24/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9207\n",
      "Epoch 00024: val_accuracy did not improve from 0.90167\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.90167\n",
      "415/415 [==============================] - 60s 145ms/step - loss: 0.2302 - accuracy: 0.9207 - val_loss: 0.3185 - val_accuracy: 0.8633\n",
      "Epoch 25/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9255\n",
      "Epoch 00025: val_accuracy did not improve from 0.90167\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.90167\n",
      "415/415 [==============================] - 60s 144ms/step - loss: 0.2158 - accuracy: 0.9255 - val_loss: 0.4500 - val_accuracy: 0.8567\n",
      "Epoch 26/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.9265\n",
      "Epoch 00026: val_accuracy did not improve from 0.90167\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.90167\n",
      "415/415 [==============================] - 60s 144ms/step - loss: 0.2165 - accuracy: 0.9265 - val_loss: 0.3113 - val_accuracy: 0.8783\n",
      "Epoch 27/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9229\n",
      "Epoch 00027: val_accuracy improved from 0.90167 to 0.91500, saving model to weights-improvement-27-0.92.hdf5\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.90167 to 0.91500, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 60s 144ms/step - loss: 0.2204 - accuracy: 0.9229 - val_loss: 0.2135 - val_accuracy: 0.9150\n",
      "Epoch 28/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.2083 - accuracy: 0.9294\n",
      "Epoch 00028: val_accuracy did not improve from 0.91500\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.91500\n",
      "415/415 [==============================] - 60s 145ms/step - loss: 0.2083 - accuracy: 0.9294 - val_loss: 0.2671 - val_accuracy: 0.9000\n",
      "Epoch 29/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.9301\n",
      "Epoch 00029: val_accuracy improved from 0.91500 to 0.91667, saving model to weights-improvement-29-0.92.hdf5\n",
      "\n",
      "Epoch 00029: val_accuracy improved from 0.91500 to 0.91667, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 60s 145ms/step - loss: 0.1957 - accuracy: 0.9301 - val_loss: 0.1985 - val_accuracy: 0.9167\n",
      "Epoch 30/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9361\n",
      "Epoch 00030: val_accuracy did not improve from 0.91667\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.91667\n",
      "415/415 [==============================] - 60s 145ms/step - loss: 0.1873 - accuracy: 0.9361 - val_loss: 0.2317 - val_accuracy: 0.9133\n",
      "Epoch 31/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1813 - accuracy: 0.9357\n",
      "Epoch 00031: val_accuracy did not improve from 0.91667\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.91667\n",
      "415/415 [==============================] - 60s 144ms/step - loss: 0.1813 - accuracy: 0.9357 - val_loss: 0.2716 - val_accuracy: 0.8967\n",
      "Epoch 32/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9417\n",
      "Epoch 00032: val_accuracy did not improve from 0.91667\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.91667\n",
      "415/415 [==============================] - 63s 153ms/step - loss: 0.1682 - accuracy: 0.9417 - val_loss: 0.3276 - val_accuracy: 0.8917\n",
      "Epoch 33/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9357\n",
      "Epoch 00033: val_accuracy did not improve from 0.91667\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.91667\n",
      "415/415 [==============================] - 68s 164ms/step - loss: 0.1717 - accuracy: 0.9357 - val_loss: 0.2425 - val_accuracy: 0.9067\n",
      "Epoch 34/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9436\n",
      "Epoch 00034: val_accuracy did not improve from 0.91667\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.91667\n",
      "415/415 [==============================] - 66s 160ms/step - loss: 0.1632 - accuracy: 0.9436 - val_loss: 0.2663 - val_accuracy: 0.9017\n",
      "Epoch 35/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9390\n",
      "Epoch 00035: val_accuracy improved from 0.91667 to 0.91833, saving model to weights-improvement-35-0.92.hdf5\n",
      "\n",
      "Epoch 00035: val_accuracy improved from 0.91667 to 0.91833, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 65s 157ms/step - loss: 0.1624 - accuracy: 0.9390 - val_loss: 0.2518 - val_accuracy: 0.9183\n",
      "Epoch 36/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1764 - accuracy: 0.9388\n",
      "Epoch 00036: val_accuracy did not improve from 0.91833\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.91833\n",
      "415/415 [==============================] - 66s 159ms/step - loss: 0.1764 - accuracy: 0.9388 - val_loss: 0.2234 - val_accuracy: 0.9150\n",
      "Epoch 37/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.9482\n",
      "Epoch 00037: val_accuracy did not improve from 0.91833\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.91833\n",
      "415/415 [==============================] - 64s 155ms/step - loss: 0.1459 - accuracy: 0.9482 - val_loss: 0.2648 - val_accuracy: 0.9033\n",
      "Epoch 38/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9489\n",
      "Epoch 00038: val_accuracy did not improve from 0.91833\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.91833\n",
      "415/415 [==============================] - 65s 156ms/step - loss: 0.1404 - accuracy: 0.9489 - val_loss: 0.2793 - val_accuracy: 0.9050\n",
      "Epoch 39/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9525\n",
      "Epoch 00039: val_accuracy improved from 0.91833 to 0.93000, saving model to weights-improvement-39-0.93.hdf5\n",
      "\n",
      "Epoch 00039: val_accuracy improved from 0.91833 to 0.93000, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.1308 - accuracy: 0.9525 - val_loss: 0.2177 - val_accuracy: 0.9300\n",
      "Epoch 40/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9513\n",
      "Epoch 00040: val_accuracy did not improve from 0.93000\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.93000\n",
      "415/415 [==============================] - 65s 157ms/step - loss: 0.1431 - accuracy: 0.9513 - val_loss: 0.2851 - val_accuracy: 0.9050\n",
      "Epoch 41/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1558 - accuracy: 0.9465\n",
      "Epoch 00041: val_accuracy did not improve from 0.93000\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.93000\n",
      "415/415 [==============================] - 65s 155ms/step - loss: 0.1558 - accuracy: 0.9465 - val_loss: 0.3533 - val_accuracy: 0.8917\n",
      "Epoch 42/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9564\n",
      "Epoch 00042: val_accuracy did not improve from 0.93000\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.93000\n",
      "415/415 [==============================] - 63s 152ms/step - loss: 0.1253 - accuracy: 0.9564 - val_loss: 0.2668 - val_accuracy: 0.9083\n",
      "Epoch 43/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9549\n",
      "Epoch 00043: val_accuracy improved from 0.93000 to 0.93333, saving model to weights-improvement-43-0.93.hdf5\n",
      "\n",
      "Epoch 00043: val_accuracy improved from 0.93000 to 0.93333, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 64s 155ms/step - loss: 0.1259 - accuracy: 0.9549 - val_loss: 0.2054 - val_accuracy: 0.9333\n",
      "Epoch 44/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.9489\n",
      "Epoch 00044: val_accuracy did not improve from 0.93333\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.93333\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.1328 - accuracy: 0.9489 - val_loss: 0.4361 - val_accuracy: 0.8783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9588\n",
      "Epoch 00045: val_accuracy did not improve from 0.93333\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.93333\n",
      "415/415 [==============================] - 62s 149ms/step - loss: 0.1300 - accuracy: 0.9588 - val_loss: 0.3785 - val_accuracy: 0.8967\n",
      "Epoch 46/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9545\n",
      "Epoch 00046: val_accuracy did not improve from 0.93333\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.93333\n",
      "415/415 [==============================] - 63s 151ms/step - loss: 0.1275 - accuracy: 0.9545 - val_loss: 0.4361 - val_accuracy: 0.8883\n",
      "Epoch 47/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9578\n",
      "Epoch 00047: val_accuracy did not improve from 0.93333\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.93333\n",
      "415/415 [==============================] - 62s 148ms/step - loss: 0.1155 - accuracy: 0.9578 - val_loss: 0.2882 - val_accuracy: 0.9100\n",
      "Epoch 48/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9588\n",
      "Epoch 00048: val_accuracy did not improve from 0.93333\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.93333\n",
      "415/415 [==============================] - 63s 151ms/step - loss: 0.1157 - accuracy: 0.9588 - val_loss: 0.3125 - val_accuracy: 0.9033\n",
      "Epoch 49/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1228 - accuracy: 0.9557\n",
      "Epoch 00049: val_accuracy did not improve from 0.93333\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.93333\n",
      "415/415 [==============================] - 63s 151ms/step - loss: 0.1228 - accuracy: 0.9557 - val_loss: 0.2818 - val_accuracy: 0.9067\n",
      "Epoch 50/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.9605\n",
      "Epoch 00050: val_accuracy did not improve from 0.93333\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.93333\n",
      "415/415 [==============================] - 65s 156ms/step - loss: 0.1179 - accuracy: 0.9605 - val_loss: 0.2733 - val_accuracy: 0.9067\n",
      "Epoch 51/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9627\n",
      "Epoch 00051: val_accuracy improved from 0.93333 to 0.94333, saving model to weights-improvement-51-0.94.hdf5\n",
      "\n",
      "Epoch 00051: val_accuracy improved from 0.93333 to 0.94333, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 62s 150ms/step - loss: 0.1014 - accuracy: 0.9627 - val_loss: 0.2138 - val_accuracy: 0.9433\n",
      "Epoch 52/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9617\n",
      "Epoch 00052: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 63s 152ms/step - loss: 0.1029 - accuracy: 0.9617 - val_loss: 0.3103 - val_accuracy: 0.8983\n",
      "Epoch 53/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9617\n",
      "Epoch 00053: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 62s 150ms/step - loss: 0.1101 - accuracy: 0.9617 - val_loss: 0.2338 - val_accuracy: 0.9317\n",
      "Epoch 54/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9627\n",
      "Epoch 00054: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 153ms/step - loss: 0.1119 - accuracy: 0.9627 - val_loss: 0.4216 - val_accuracy: 0.8817\n",
      "Epoch 55/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.9569\n",
      "Epoch 00055: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 63s 152ms/step - loss: 0.1237 - accuracy: 0.9569 - val_loss: 0.2952 - val_accuracy: 0.8917\n",
      "Epoch 56/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9651\n",
      "Epoch 00056: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0952 - accuracy: 0.9651 - val_loss: 0.2052 - val_accuracy: 0.9333\n",
      "Epoch 57/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9740\n",
      "Epoch 00057: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 62s 149ms/step - loss: 0.0762 - accuracy: 0.9740 - val_loss: 0.2370 - val_accuracy: 0.9167\n",
      "Epoch 58/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9704\n",
      "Epoch 00058: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 62s 150ms/step - loss: 0.0861 - accuracy: 0.9704 - val_loss: 0.3628 - val_accuracy: 0.9133\n",
      "Epoch 59/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9631\n",
      "Epoch 00059: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.1018 - accuracy: 0.9631 - val_loss: 0.2019 - val_accuracy: 0.9250\n",
      "Epoch 60/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9694\n",
      "Epoch 00060: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 155ms/step - loss: 0.1026 - accuracy: 0.9694 - val_loss: 0.2219 - val_accuracy: 0.9250\n",
      "Epoch 61/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9571\n",
      "Epoch 00061: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 65s 155ms/step - loss: 0.1265 - accuracy: 0.9571 - val_loss: 0.2500 - val_accuracy: 0.9200\n",
      "Epoch 62/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9660\n",
      "Epoch 00062: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 65s 155ms/step - loss: 0.0949 - accuracy: 0.9660 - val_loss: 0.2402 - val_accuracy: 0.9317\n",
      "Epoch 63/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9704\n",
      "Epoch 00063: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 153ms/step - loss: 0.0858 - accuracy: 0.9704 - val_loss: 0.2549 - val_accuracy: 0.9367\n",
      "Epoch 64/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9682\n",
      "Epoch 00064: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 65s 156ms/step - loss: 0.0903 - accuracy: 0.9682 - val_loss: 0.2621 - val_accuracy: 0.9083\n",
      "Epoch 65/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9680\n",
      "Epoch 00065: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 66s 158ms/step - loss: 0.0956 - accuracy: 0.9680 - val_loss: 0.2257 - val_accuracy: 0.9317\n",
      "Epoch 66/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9733\n",
      "Epoch 00066: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 63s 152ms/step - loss: 0.0833 - accuracy: 0.9733 - val_loss: 0.2215 - val_accuracy: 0.9267\n",
      "Epoch 67/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9713\n",
      "Epoch 00067: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 62s 149ms/step - loss: 0.0764 - accuracy: 0.9713 - val_loss: 0.4571 - val_accuracy: 0.8750\n",
      "Epoch 68/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9704\n",
      "Epoch 00068: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 153ms/step - loss: 0.0847 - accuracy: 0.9704 - val_loss: 0.4764 - val_accuracy: 0.8800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9708\n",
      "Epoch 00069: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 67s 162ms/step - loss: 0.0872 - accuracy: 0.9708 - val_loss: 0.3016 - val_accuracy: 0.9083\n",
      "Epoch 70/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9735\n",
      "Epoch 00070: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 155ms/step - loss: 0.0911 - accuracy: 0.9735 - val_loss: 0.3039 - val_accuracy: 0.9050\n",
      "Epoch 71/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9742\n",
      "Epoch 00071: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 153ms/step - loss: 0.0741 - accuracy: 0.9742 - val_loss: 0.2765 - val_accuracy: 0.9250\n",
      "Epoch 72/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9788\n",
      "Epoch 00072: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0634 - accuracy: 0.9788 - val_loss: 0.2869 - val_accuracy: 0.9300\n",
      "Epoch 73/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9692\n",
      "Epoch 00073: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 153ms/step - loss: 0.0900 - accuracy: 0.9692 - val_loss: 0.2421 - val_accuracy: 0.9300\n",
      "Epoch 74/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9704\n",
      "Epoch 00074: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 65s 157ms/step - loss: 0.0820 - accuracy: 0.9704 - val_loss: 0.3599 - val_accuracy: 0.9083\n",
      "Epoch 75/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9742\n",
      "Epoch 00075: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 65s 156ms/step - loss: 0.0804 - accuracy: 0.9742 - val_loss: 0.3453 - val_accuracy: 0.9150\n",
      "Epoch 76/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9771\n",
      "Epoch 00076: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 66s 159ms/step - loss: 0.0585 - accuracy: 0.9771 - val_loss: 0.2630 - val_accuracy: 0.9350\n",
      "Epoch 77/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9723\n",
      "Epoch 00077: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 155ms/step - loss: 0.0813 - accuracy: 0.9723 - val_loss: 0.3267 - val_accuracy: 0.9067\n",
      "Epoch 78/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9783\n",
      "Epoch 00078: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0693 - accuracy: 0.9783 - val_loss: 0.2684 - val_accuracy: 0.9250\n",
      "Epoch 79/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9730\n",
      "Epoch 00079: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 65s 156ms/step - loss: 0.0785 - accuracy: 0.9730 - val_loss: 0.3173 - val_accuracy: 0.9217\n",
      "Epoch 80/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9773\n",
      "Epoch 00080: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 65s 156ms/step - loss: 0.0670 - accuracy: 0.9773 - val_loss: 0.3134 - val_accuracy: 0.9133\n",
      "Epoch 81/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9701\n",
      "Epoch 00081: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 67s 161ms/step - loss: 0.0987 - accuracy: 0.9701 - val_loss: 0.2421 - val_accuracy: 0.9283\n",
      "Epoch 82/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9807\n",
      "Epoch 00082: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0528 - accuracy: 0.9807 - val_loss: 0.3947 - val_accuracy: 0.9067\n",
      "Epoch 83/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9740\n",
      "Epoch 00083: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0769 - accuracy: 0.9740 - val_loss: 0.2640 - val_accuracy: 0.9183\n",
      "Epoch 84/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9749\n",
      "Epoch 00084: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 65s 156ms/step - loss: 0.0675 - accuracy: 0.9749 - val_loss: 0.2317 - val_accuracy: 0.9317\n",
      "Epoch 85/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9766\n",
      "Epoch 00085: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 155ms/step - loss: 0.0636 - accuracy: 0.9766 - val_loss: 0.2694 - val_accuracy: 0.9217\n",
      "Epoch 86/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9783\n",
      "Epoch 00086: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0651 - accuracy: 0.9783 - val_loss: 0.2271 - val_accuracy: 0.9300\n",
      "Epoch 87/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.9781\n",
      "Epoch 00087: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0607 - accuracy: 0.9781 - val_loss: 0.3249 - val_accuracy: 0.9167\n",
      "Epoch 88/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9757\n",
      "Epoch 00088: val_accuracy did not improve from 0.94333\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.94333\n",
      "415/415 [==============================] - 64s 155ms/step - loss: 0.0794 - accuracy: 0.9757 - val_loss: 0.2801 - val_accuracy: 0.9433\n",
      "Epoch 89/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9807\n",
      "Epoch 00089: val_accuracy improved from 0.94333 to 0.95000, saving model to weights-improvement-89-0.95.hdf5\n",
      "\n",
      "Epoch 00089: val_accuracy improved from 0.94333 to 0.95000, saving model to best_weights.hdf5\n",
      "415/415 [==============================] - 65s 156ms/step - loss: 0.0538 - accuracy: 0.9807 - val_loss: 0.2028 - val_accuracy: 0.9500\n",
      "Epoch 90/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9773\n",
      "Epoch 00090: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 64s 155ms/step - loss: 0.0664 - accuracy: 0.9773 - val_loss: 0.3343 - val_accuracy: 0.9183\n",
      "Epoch 91/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9773\n",
      "Epoch 00091: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0684 - accuracy: 0.9773 - val_loss: 0.2580 - val_accuracy: 0.9367\n",
      "Epoch 92/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9827\n",
      "Epoch 00092: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 65s 155ms/step - loss: 0.0548 - accuracy: 0.9827 - val_loss: 0.3262 - val_accuracy: 0.9217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9740\n",
      "Epoch 00093: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 65s 157ms/step - loss: 0.0757 - accuracy: 0.9740 - val_loss: 0.2449 - val_accuracy: 0.9367\n",
      "Epoch 94/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9802\n",
      "Epoch 00094: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0580 - accuracy: 0.9802 - val_loss: 0.2256 - val_accuracy: 0.9483\n",
      "Epoch 95/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9800\n",
      "Epoch 00095: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 63s 152ms/step - loss: 0.0674 - accuracy: 0.9800 - val_loss: 0.3613 - val_accuracy: 0.9233\n",
      "Epoch 96/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9829\n",
      "Epoch 00096: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0532 - accuracy: 0.9829 - val_loss: 0.3732 - val_accuracy: 0.9033\n",
      "Epoch 97/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9781\n",
      "Epoch 00097: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 64s 154ms/step - loss: 0.0643 - accuracy: 0.9781 - val_loss: 0.4482 - val_accuracy: 0.9033\n",
      "Epoch 98/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9817\n",
      "Epoch 00098: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 67s 160ms/step - loss: 0.0587 - accuracy: 0.9817 - val_loss: 0.5270 - val_accuracy: 0.8867\n",
      "Epoch 99/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9778\n",
      "Epoch 00099: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 63s 151ms/step - loss: 0.0659 - accuracy: 0.9778 - val_loss: 0.3211 - val_accuracy: 0.9217\n",
      "Epoch 100/100\n",
      "415/415 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9817\n",
      "Epoch 00100: val_accuracy did not improve from 0.95000\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.95000\n",
      "415/415 [==============================] - 64s 153ms/step - loss: 0.0526 - accuracy: 0.9817 - val_loss: 0.2809 - val_accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "# 'train_batches' is the training data\n",
    "# 'valid_batches' is the validation data\n",
    "# The model will train for 100 epochs\n",
    "# 'verbose=1' means the training process will print out metrics after each epoch\n",
    "# 'callbacks_list' is a list of callbacks to apply during training\n",
    "# The training process will return a 'History' object which we store in 'history'\n",
    "# The 'History' object has a record of training loss values and metrics values at successive epochs, \n",
    "# as well as validation loss values and validation metrics values (if applicable)\n",
    "history = model.fit(\n",
    "    train_batches, \n",
    "    validation_data = valid_batches, \n",
    "    epochs = 100, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db106544",
   "metadata": {},
   "source": [
    "## Testing model on our Test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c8b3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to make predictions on the test set\n",
    "# 'test_batches' is the test data\n",
    "# 'steps=len(test_batches)' means the prediction process will run for a number of steps equal to the total number of batches in the test set\n",
    "# 'verbose=0' means the prediction process will not print out any progress messages\n",
    "# The prediction process will return an array of predictions which we store in 'predictions'\n",
    "predictions = model.predict(\n",
    "    x=test_batches, \n",
    "    steps=len(test_batches), \n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aac08245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Function to print and plot the confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    cm : array-like\n",
    "        The confusion matrix.\n",
    "    classes : list\n",
    "        The list of class labels.\n",
    "    normalize : bool, optional\n",
    "        Whether to normalize the confusion matrix. Default is False.\n",
    "    title : str, optional\n",
    "        The title of the plot. Default is 'Confusion matrix'.\n",
    "    cmap : colormap, optional\n",
    "        The colormap to be used. Default is 'Blues'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the confusion matrix as an image\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    \n",
    "    # Set the title of the plot\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Display a colorbar\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Create tick marks at each class label\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    \n",
    "    # Label the x-axis with the class labels, rotated 45 degrees\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    \n",
    "    # Label the y-axis with the class labels\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    # Normalize the confusion matrix if requested\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    print(cm)\n",
    "    \n",
    "    # Determine the threshold for coloring the text\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # Loop over each cell in the confusion matrix\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        # Add the value of the cell as text in the center of the cell\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # Adjust the layout to fit everything\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Label the y-axis\n",
    "    plt.ylabel('True label')\n",
    "    \n",
    "    # Label the x-axis\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20ac0f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 44  66  46 102  35  62 115  38  88  42  80  76]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the confusion matrix\n",
    "# 'y_true=test_batches.classes' are the true labels\n",
    "# 'y_pred=np.argmax(predictions, axis=-1)' are the predicted labels\n",
    "# 'np.argmax(predictions, axis=-1)' returns the indices of the maximum values along an axis\n",
    "cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))\n",
    "\n",
    "# List of class labels for the confusion matrix\n",
    "cm_plot_labels = ['Black-grass', 'Charlock', 'Cleavers',\n",
    "        'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n",
    "        'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n",
    "\n",
    "# Plot the confusion matrix using the previously defined function\n",
    "# 'cm=cm' is the confusion matrix to be plotted\n",
    "# 'classes=cm_plot_labels' are the class labels\n",
    "# 'title='Confusion Matrix'' is the title of the plot\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85ad5ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that the test data has wrong labels. You can check the data on kaggle: https://www.kaggle.com/c/plant-seedlings-classification/data\n"
     ]
    }
   ],
   "source": [
    "print(f'It seems that the test data might have incorrect labels. You can verify the data on Kaggle using the following link: {link}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
