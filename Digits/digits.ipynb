{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a185dad",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade51eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: # For inline backend\n"
     ]
    }
   ],
   "source": [
    "# Importing frequently used libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import tensorflow as tf  # For machine learning and neural networks\n",
    "import pickle  # For serializing and de-serializing Python object structures\n",
    "from tensorflow import keras  # For neural networks\n",
    "from tensorflow.keras.models import Sequential, Model, load_model  # For sequential and functional API models\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout, InputLayer  # For different types of layers in a neural network\n",
    "from tensorflow.keras.optimizers import Adam  # For the Adam optimizer\n",
    "from tensorflow.keras.metrics import categorical_crossentropy, MeanSquaredError  # For different types of loss functions\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For real-time data augmentation\n",
    "from tensorflow.keras.utils import to_categorical  # For one-hot encoding\n",
    "from tensorflow.keras.datasets import cifar100  # CIFAR-100 dataset\n",
    "from sklearn.metrics import confusion_matrix  # For creating a confusion matrix\n",
    "from sklearn.model_selection import train_test_split  # For splitting the data into train and test sets\n",
    "from sklearn.model_selection import KFold  # For K-Fold cross validation\n",
    "import itertools  # For creating iterators for efficient looping\n",
    "import os  # For interacting with the OS\n",
    "import shutil  # For high-level file operations\n",
    "import torch  # PyTorch library for machine learning\n",
    "import random  # For generating random numbers\n",
    "import glob  # For finding all the pathnames matching a specified pattern\n",
    "import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations in Python\n",
    "import warnings  # For warning control\n",
    "import tensorflow.keras.applications.efficientnet as efn  # For EfficientNet model\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)  # Ignore future warnings\n",
    "%matplotlib inline  # For inline backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5770009",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e8b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the working directory to the location of the datasets\n",
    "os.chdir(r'C:\\Users\\RezaHardMan\\Documents\\Python projects\\datasets\\digits')\n",
    "\n",
    "# Reading the training dataset from a CSV file\n",
    "dt_train = pd.read_csv('train.csv')\n",
    "\n",
    "# Reading the testing dataset from a CSV file\n",
    "dt_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Reading the sample submission dataset from a CSV file\n",
    "dt_sample = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Converting the training dataset into a DataFrame for easier manipulation\n",
    "df_train = pd.DataFrame(dt_train)\n",
    "\n",
    "# Converting the testing dataset into a DataFrame for easier manipulation\n",
    "df_test = pd.DataFrame(dt_test)\n",
    "\n",
    "# Converting the sample submission dataset into a DataFrame for easier manipulation\n",
    "df_sample = pd.DataFrame(dt_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c76f8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Displaying info of the training dataset\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e35c1a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0        0       0       0       0       0       0       0       0       0   \n",
       "1        0       0       0       0       0       0       0       0       0   \n",
       "2        0       0       0       0       0       0       0       0       0   \n",
       "3        0       0       0       0       0       0       0       0       0   \n",
       "4        0       0       0       0       0       0       0       0       0   \n",
       "5        0       0       0       0       0       0       0       0       0   \n",
       "6        0       0       0       0       0       0       0       0       0   \n",
       "7        0       0       0       0       0       0       0       0       0   \n",
       "8        0       0       0       0       0       0       0       0       0   \n",
       "9        0       0       0       0       0       0       0       0       0   \n",
       "10       0       0       0       0       0       0       0       0       0   \n",
       "11       0       0       0       0       0       0       0       0       0   \n",
       "12       0       0       0       0       0       0       0       0       0   \n",
       "13       0       0       0       0       0       0       0       0       0   \n",
       "14       0       0       0       0       0       0       0       0       0   \n",
       "15       0       0       0       0       0       0       0       0       0   \n",
       "16       0       0       0       0       0       0       0       0       0   \n",
       "17       0       0       0       0       0       0       0       0       0   \n",
       "18       0       0       0       0       0       0       0       0       0   \n",
       "19       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "    pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0        0  ...         0         0         0         0         0         0   \n",
       "1        0  ...         0         0         0         0         0         0   \n",
       "2        0  ...         0         0         0         0         0         0   \n",
       "3        0  ...         0         0         0         0         0         0   \n",
       "4        0  ...         0         0         0         0         0         0   \n",
       "5        0  ...         0         0         0         0         0         0   \n",
       "6        0  ...         0         0         0         0         0         0   \n",
       "7        0  ...         0         0         0         0         0         0   \n",
       "8        0  ...         0         0         0         0         0         0   \n",
       "9        0  ...         0         0         0         0         0         0   \n",
       "10       0  ...         0         0         0         0         0         0   \n",
       "11       0  ...         0         0         0         0         0         0   \n",
       "12       0  ...         0         0         0         0         0         0   \n",
       "13       0  ...         0         0         0         0         0         0   \n",
       "14       0  ...         0         0         0         0         0         0   \n",
       "15       0  ...         0         0         0         0         0         0   \n",
       "16       0  ...         0         0         0         0         0         0   \n",
       "17       0  ...         0         0         0         0         0         0   \n",
       "18       0  ...         0         0         0         0         0         0   \n",
       "19       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "    pixel780  pixel781  pixel782  pixel783  \n",
       "0          0         0         0         0  \n",
       "1          0         0         0         0  \n",
       "2          0         0         0         0  \n",
       "3          0         0         0         0  \n",
       "4          0         0         0         0  \n",
       "5          0         0         0         0  \n",
       "6          0         0         0         0  \n",
       "7          0         0         0         0  \n",
       "8          0         0         0         0  \n",
       "9          0         0         0         0  \n",
       "10         0         0         0         0  \n",
       "11         0         0         0         0  \n",
       "12         0         0         0         0  \n",
       "13         0         0         0         0  \n",
       "14         0         0         0         0  \n",
       "15         0         0         0         0  \n",
       "16         0         0         0         0  \n",
       "17         0         0         0         0  \n",
       "18         0         0         0         0  \n",
       "19         0         0         0         0  \n",
       "\n",
       "[20 rows x 784 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first 20 rows of the testing dataset\n",
    "dt_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436c754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training data\n",
    "# Dropping the 'label' column from the training data as it is the target variable\n",
    "X_train = dt_train.drop('label', axis=1)\n",
    "\n",
    "# The test data is already prepared and doesn't need to be changed\n",
    "X_test = dt_test\n",
    "\n",
    "# Extracting the 'label' column from the training and testing data to create the target variable\n",
    "y_train = dt_train['label']\n",
    "y_test = dt_sample['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07d4d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the training data to fit the model\n",
    "# The '-1' in reshape function is used when you aren't sure about the dimension and you want numpy to calculate it for you\n",
    "# Here we are reshaping a 2D array into a 3D array\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "\n",
    "# Reshaping the testing data to fit the model\n",
    "X_test = X_test.values.reshape(-1,28,28,1)\n",
    "\n",
    "# Converting the target variable into categorical variables\n",
    "# 'num_classes = 10' indicates that there are 10 classes in the target variable\n",
    "y_train = to_categorical(y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034764c",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3759f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        # Boolean. Set input mean to 0 over the dataset, feature-wise\n",
    "        featurewise_center=True,  \n",
    "        \n",
    "        # Boolean. Set each sample mean to 0\n",
    "        samplewise_center=True,  \n",
    "        \n",
    "        # Boolean. Divide inputs by std of the dataset, feature-wise\n",
    "        featurewise_std_normalization=True,  \n",
    "        \n",
    "        # Boolean. Divide each input by its std\n",
    "        samplewise_std_normalization=True,   \n",
    "        \n",
    "        # Int. Degree range for random rotations\n",
    "        rotation_range=10,  \n",
    "        \n",
    "        # Float. Range for random zoom\n",
    "        zoom_range = 0.1,\n",
    "        \n",
    "        # Float (fraction of total width). Range for random horizontal shifts\n",
    "        width_shift_range=0.1,  \n",
    "        \n",
    "        # Float (fraction of total height). Range for random vertical shifts\n",
    "        height_shift_range=0.1,\n",
    "        \n",
    "        # Float. Fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc38bcc",
   "metadata": {},
   "source": [
    "## Training data with my semi-simple Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8079ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        25632     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              6423552   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,460,394\n",
      "Trainable params: 6,460,330\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Defining the model architecture\n",
    "model2 = Sequential([\n",
    "    # First convolutional layer with 32 filters of size 5x5, ReLU activation function, same padding and input shape of 28x28x1\n",
    "    Conv2D(filters=32, kernel_size=(5, 5), activation='relu', padding = 'same', input_shape=(28,28,1)),\n",
    "    \n",
    "    # Second convolutional layer with 32 filters of size 5x5, ReLU activation function and same padding\n",
    "    Conv2D(filters=32, kernel_size=(5, 5), activation='relu', padding = 'same'),\n",
    "    \n",
    "    # Batch normalization layer to normalize the activations of the previous layer\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Max pooling layer with pool size of 2x2 to reduce the spatial dimensions of the output volume\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Repeat the same for more layers with different parameters\n",
    "    # ...\n",
    "    \n",
    "    # Flatten layer to convert the 3D outputs to 1D vector\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer with 1024 neurons and ReLU activation function\n",
    "    Dense(1024, activation = 'relu'),\n",
    "    \n",
    "    # Dropout layer with dropout rate of 0.5 to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer with 10 neurons (one for each class) and softmax activation function for multi-class classification\n",
    "    Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Printing the summary of the model\n",
    "print(model2.summary())\n",
    "\n",
    "# Compiling the model with Adam optimizer, categorical crossentropy loss function and accuracy as the evaluation metric\n",
    "model2.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Defining the filepaths for saving the weights\n",
    "filepath1=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filepath2 = \"best_weights.hdf5\"\n",
    "\n",
    "# Defining the checkpoint callbacks for saving the best weights\n",
    "checkpoint1 = ModelCheckpoint(filepath1, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint2 = ModelCheckpoint(filepath2, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# List of callbacks\n",
    "callbacks_list = [checkpoint1,checkpoint2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e099b611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RezaHardMan\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\RezaHardMan\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1177/1182 [============================>.] - ETA: 0s - loss: 0.3807 - accuracy: 0.8808\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.96071, saving model to weights-improvement-01-0.96.hdf5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.96071, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 13s 7ms/step - loss: 0.3798 - accuracy: 0.8810 - val_loss: 0.1296 - val_accuracy: 0.9607\n",
      "Epoch 2/30\n",
      "1178/1182 [============================>.] - ETA: 0s - loss: 0.1352 - accuracy: 0.9577\n",
      "Epoch 00002: val_accuracy improved from 0.96071 to 0.97476, saving model to weights-improvement-02-0.97.hdf5\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.96071 to 0.97476, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 10s 8ms/step - loss: 0.1351 - accuracy: 0.9577 - val_loss: 0.0728 - val_accuracy: 0.9748\n",
      "Epoch 3/30\n",
      "1180/1182 [============================>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.9688\n",
      "Epoch 00003: val_accuracy improved from 0.97476 to 0.98333, saving model to weights-improvement-03-0.98.hdf5\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.97476 to 0.98333, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 10s 9ms/step - loss: 0.0986 - accuracy: 0.9689 - val_loss: 0.0557 - val_accuracy: 0.9833\n",
      "Epoch 4/30\n",
      "1182/1182 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9751\n",
      "Epoch 00004: val_accuracy improved from 0.98333 to 0.98381, saving model to weights-improvement-04-0.98.hdf5\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.98333 to 0.98381, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 11s 9ms/step - loss: 0.0813 - accuracy: 0.9751 - val_loss: 0.0476 - val_accuracy: 0.9838\n",
      "Epoch 5/30\n",
      "1179/1182 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9779\n",
      "Epoch 00005: val_accuracy did not improve from 0.98381\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.98381\n",
      "1182/1182 [==============================] - 11s 9ms/step - loss: 0.0715 - accuracy: 0.9778 - val_loss: 0.0583 - val_accuracy: 0.9819\n",
      "Epoch 6/30\n",
      "1182/1182 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9808\n",
      "Epoch 00006: val_accuracy did not improve from 0.98381\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.98381\n",
      "1182/1182 [==============================] - 11s 9ms/step - loss: 0.0631 - accuracy: 0.9808 - val_loss: 0.0672 - val_accuracy: 0.9790\n",
      "Epoch 7/30\n",
      "1179/1182 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9830\n",
      "Epoch 00007: val_accuracy improved from 0.98381 to 0.98500, saving model to weights-improvement-07-0.99.hdf5\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.98381 to 0.98500, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 12s 10ms/step - loss: 0.0560 - accuracy: 0.9830 - val_loss: 0.0456 - val_accuracy: 0.9850\n",
      "Epoch 8/30\n",
      "1177/1182 [============================>.] - ETA: 0s - loss: 0.0532 - accuracy: 0.9839\n",
      "Epoch 00008: val_accuracy improved from 0.98500 to 0.98643, saving model to weights-improvement-08-0.99.hdf5\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.98500 to 0.98643, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 12s 10ms/step - loss: 0.0531 - accuracy: 0.9839 - val_loss: 0.0458 - val_accuracy: 0.9864\n",
      "Epoch 9/30\n",
      "1179/1182 [============================>.] - ETA: 0s - loss: 0.0490 - accuracy: 0.9849\n",
      "Epoch 00009: val_accuracy improved from 0.98643 to 0.98738, saving model to weights-improvement-09-0.99.hdf5\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.98643 to 0.98738, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 12s 10ms/step - loss: 0.0490 - accuracy: 0.9848 - val_loss: 0.0384 - val_accuracy: 0.9874\n",
      "Epoch 10/30\n",
      "1181/1182 [============================>.] - ETA: 0s - loss: 0.0484 - accuracy: 0.9854\n",
      "Epoch 00010: val_accuracy improved from 0.98738 to 0.98905, saving model to weights-improvement-10-0.99.hdf5\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.98738 to 0.98905, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 12s 10ms/step - loss: 0.0484 - accuracy: 0.9854 - val_loss: 0.0336 - val_accuracy: 0.9890\n",
      "Epoch 11/30\n",
      "1177/1182 [============================>.] - ETA: 0s - loss: 0.0441 - accuracy: 0.9870\n",
      "Epoch 00011: val_accuracy did not improve from 0.98905\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.98905\n",
      "1182/1182 [==============================] - 12s 10ms/step - loss: 0.0443 - accuracy: 0.9869 - val_loss: 0.0314 - val_accuracy: 0.9890\n",
      "Epoch 12/30\n",
      "1177/1182 [============================>.] - ETA: 0s - loss: 0.0404 - accuracy: 0.9871\n",
      "Epoch 00012: val_accuracy improved from 0.98905 to 0.99000, saving model to weights-improvement-12-0.99.hdf5\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.98905 to 0.99000, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 12s 11ms/step - loss: 0.0406 - accuracy: 0.9870 - val_loss: 0.0372 - val_accuracy: 0.9900\n",
      "Epoch 13/30\n",
      "1179/1182 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9881\n",
      "Epoch 00013: val_accuracy did not improve from 0.99000\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.99000\n",
      "1182/1182 [==============================] - 12s 10ms/step - loss: 0.0403 - accuracy: 0.9881 - val_loss: 0.0411 - val_accuracy: 0.9879\n",
      "Epoch 14/30\n",
      "1180/1182 [============================>.] - ETA: 0s - loss: 0.0384 - accuracy: 0.9878\n",
      "Epoch 00014: val_accuracy improved from 0.99000 to 0.99095, saving model to weights-improvement-14-0.99.hdf5\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.99000 to 0.99095, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0383 - accuracy: 0.9879 - val_loss: 0.0331 - val_accuracy: 0.9910\n",
      "Epoch 15/30\n",
      "1180/1182 [============================>.] - ETA: 0s - loss: 0.0358 - accuracy: 0.9891\n",
      "Epoch 00015: val_accuracy did not improve from 0.99095\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.99095\n",
      "1182/1182 [==============================] - 12s 10ms/step - loss: 0.0358 - accuracy: 0.9891 - val_loss: 0.0374 - val_accuracy: 0.9902\n",
      "Epoch 16/30\n",
      "1177/1182 [============================>.] - ETA: 0s - loss: 0.0330 - accuracy: 0.9903\n",
      "Epoch 00016: val_accuracy did not improve from 0.99095\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.99095\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0333 - accuracy: 0.9903 - val_loss: 0.0372 - val_accuracy: 0.9883\n",
      "Epoch 17/30\n",
      "1180/1182 [============================>.] - ETA: 0s - loss: 0.0340 - accuracy: 0.9896\n",
      "Epoch 00017: val_accuracy did not improve from 0.99095\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.99095\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0340 - accuracy: 0.9896 - val_loss: 0.0402 - val_accuracy: 0.9900\n",
      "Epoch 18/30\n",
      "1181/1182 [============================>.] - ETA: 0s - loss: 0.0336 - accuracy: 0.9897\n",
      "Epoch 00018: val_accuracy improved from 0.99095 to 0.99119, saving model to weights-improvement-18-0.99.hdf5\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.99095 to 0.99119, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0336 - accuracy: 0.9897 - val_loss: 0.0300 - val_accuracy: 0.9912\n",
      "Epoch 19/30\n",
      "1181/1182 [============================>.] - ETA: 0s - loss: 0.0295 - accuracy: 0.9913\n",
      "Epoch 00019: val_accuracy improved from 0.99119 to 0.99190, saving model to weights-improvement-19-0.99.hdf5\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.99119 to 0.99190, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0294 - accuracy: 0.9913 - val_loss: 0.0308 - val_accuracy: 0.9919\n",
      "Epoch 20/30\n",
      "1177/1182 [============================>.] - ETA: 0s - loss: 0.0286 - accuracy: 0.9915\n",
      "Epoch 00020: val_accuracy did not improve from 0.99190\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.99190\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0285 - accuracy: 0.9915 - val_loss: 0.0332 - val_accuracy: 0.9902\n",
      "Epoch 21/30\n",
      "1181/1182 [============================>.] - ETA: 0s - loss: 0.0295 - accuracy: 0.9912\n",
      "Epoch 00021: val_accuracy did not improve from 0.99190\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.99190\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0294 - accuracy: 0.9912 - val_loss: 0.0329 - val_accuracy: 0.9910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "1178/1182 [============================>.] - ETA: 0s - loss: 0.0269 - accuracy: 0.9914\n",
      "Epoch 00022: val_accuracy did not improve from 0.99190\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.99190\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0269 - accuracy: 0.9914 - val_loss: 0.0293 - val_accuracy: 0.9917\n",
      "Epoch 23/30\n",
      "1179/1182 [============================>.] - ETA: 0s - loss: 0.0270 - accuracy: 0.9920\n",
      "Epoch 00023: val_accuracy did not improve from 0.99190\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.99190\n",
      "1182/1182 [==============================] - 14s 12ms/step - loss: 0.0271 - accuracy: 0.9920 - val_loss: 0.0312 - val_accuracy: 0.9902\n",
      "Epoch 24/30\n",
      "1181/1182 [============================>.] - ETA: 0s - loss: 0.0254 - accuracy: 0.9919\n",
      "Epoch 00024: val_accuracy did not improve from 0.99190\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.99190\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0253 - accuracy: 0.9919 - val_loss: 0.0283 - val_accuracy: 0.9912\n",
      "Epoch 25/30\n",
      "1178/1182 [============================>.] - ETA: 0s - loss: 0.0249 - accuracy: 0.9921\n",
      "Epoch 00025: val_accuracy did not improve from 0.99190\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.99190\n",
      "1182/1182 [==============================] - 14s 12ms/step - loss: 0.0250 - accuracy: 0.9921 - val_loss: 0.0257 - val_accuracy: 0.9917\n",
      "Epoch 26/30\n",
      "1179/1182 [============================>.] - ETA: 0s - loss: 0.0243 - accuracy: 0.9927\n",
      "Epoch 00026: val_accuracy did not improve from 0.99190\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.99190\n",
      "1182/1182 [==============================] - 14s 12ms/step - loss: 0.0244 - accuracy: 0.9927 - val_loss: 0.0287 - val_accuracy: 0.9912\n",
      "Epoch 27/30\n",
      "1177/1182 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9930\n",
      "Epoch 00027: val_accuracy did not improve from 0.99190\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.99190\n",
      "1182/1182 [==============================] - 14s 12ms/step - loss: 0.0226 - accuracy: 0.9930 - val_loss: 0.0312 - val_accuracy: 0.9917\n",
      "Epoch 28/30\n",
      "1180/1182 [============================>.] - ETA: 0s - loss: 0.0240 - accuracy: 0.9924\n",
      "Epoch 00028: val_accuracy improved from 0.99190 to 0.99214, saving model to weights-improvement-28-0.99.hdf5\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.99190 to 0.99214, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 15s 12ms/step - loss: 0.0240 - accuracy: 0.9924 - val_loss: 0.0264 - val_accuracy: 0.9921\n",
      "Epoch 29/30\n",
      "1178/1182 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9929\n",
      "Epoch 00029: val_accuracy did not improve from 0.99214\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.99214\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0221 - accuracy: 0.9929 - val_loss: 0.0295 - val_accuracy: 0.9907\n",
      "Epoch 30/30\n",
      "1178/1182 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9931\n",
      "Epoch 00030: val_accuracy improved from 0.99214 to 0.99286, saving model to weights-improvement-30-0.99.hdf5\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.99214 to 0.99286, saving model to best_weights.hdf5\n",
      "1182/1182 [==============================] - 13s 11ms/step - loss: 0.0223 - accuracy: 0.9931 - val_loss: 0.0224 - val_accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "# Calculating the number of steps per epoch for training\n",
    "# This is done by dividing the number of training samples (90% of total) by the batch size and rounding up\n",
    "TRAIN_STEPS_PER_EPOCH = np.ceil((len(X_train)*0.9/32))\n",
    "\n",
    "# Calculating the number of steps per epoch for validation\n",
    "# This is done by dividing the number of validation samples (10% of total) by the batch size and rounding up\n",
    "VAL_STEPS_PER_EPOCH = np.ceil((len(X_train)*0.1/8))\n",
    "\n",
    "# Fitting the model to the data using the ImageDataGenerator\n",
    "# The 'subset' parameter in datagen.flow allows us to specify which subset of the data to use (training or validation)\n",
    "history = model2.fit(datagen.flow(X_train, y_train, batch_size=32, subset='training'),\n",
    "                     validation_data=datagen.flow(X_train, y_train, batch_size=8, subset='validation'),\n",
    "                     steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n",
    "                     validation_steps = VAL_STEPS_PER_EPOCH,\n",
    "                     epochs = 30, verbose=1, shuffle=True, \n",
    "                     callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "882c2dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2411 2880 2866 2798 2477 2506 2811 2832 3783 2636]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the model's predictions on the test data\n",
    "y_pred = model2.predict(X_test)\n",
    "\n",
    "# Convert the predictions from probabilities to class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Import the required function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(cm)\n",
    "# Import the required libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a heatmap from the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It has been observed that all the labels in the sample are 0. Therefore, it is not feasible to evaluate our model using this test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
