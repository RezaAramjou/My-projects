{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608fcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import tensorflow.keras.applications.efficientnet as efn\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a51160",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\RezaHardMan\\Documents\\Python projects\\datasets\\age and gender detection')\n",
    "data = pd.read_csv(\"age_gender.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "323508d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>img_name</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219203650636.jpg.chip.jpg</td>\n",
       "      <td>129 128 128 126 127 130 133 135 139 142 145 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219222752047.jpg.chip.jpg</td>\n",
       "      <td>164 74 111 168 169 171 175 182 184 188 193 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219222832191.jpg.chip.jpg</td>\n",
       "      <td>67 70 71 70 69 67 70 79 90 103 116 132 145 155...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161220144911423.jpg.chip.jpg</td>\n",
       "      <td>193 197 198 200 199 200 202 203 204 205 208 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161220144914327.jpg.chip.jpg</td>\n",
       "      <td>202 205 209 210 209 209 210 211 212 214 218 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161220144957407.jpg.chip.jpg</td>\n",
       "      <td>195 198 200 200 198 198 199 199 198 197 197 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161220145040127.jpg.chip.jpg</td>\n",
       "      <td>208 216 217 219 222 223 222 221 220 220 221 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170109191125532.jpg.chip.jpg</td>\n",
       "      <td>99 142 169 177 179 181 183 186 187 186 191 190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219222749039.jpg.chip.jpg</td>\n",
       "      <td>127 127 133 140 143 148 152 157 160 165 172 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170109191209991.jpg.chip.jpg</td>\n",
       "      <td>199 211 211 214 216 216 219 221 222 224 219 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170109192102236.jpg.chip.jpg</td>\n",
       "      <td>136 138 145 143 152 160 162 169 178 182 192 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170109192222822.jpg.chip.jpg</td>\n",
       "      <td>253 253 253 253 252 251 250 209 228 250 221 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170109193535757.jpg.chip.jpg</td>\n",
       "      <td>223 222 226 227 229 228 228 226 222 214 199 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110212743721.jpg.chip.jpg</td>\n",
       "      <td>181 185 186 185 182 180 180 177 183 177 175 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110213009014.jpg.chip.jpg</td>\n",
       "      <td>167 172 175 174 172 170 172 173 171 172 172 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110213415212.jpg.chip.jpg</td>\n",
       "      <td>99 122 138 147 148 149 149 150 153 157 162 166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110213523297.jpg.chip.jpg</td>\n",
       "      <td>49 63 84 95 105 108 112 115 122 124 132 135 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170109191453449.jpg.chip.jpg</td>\n",
       "      <td>226 225 223 166 136 167 170 173 176 177 180 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20170116194202388.jpg.chip.jpg</td>\n",
       "      <td>238 187 173 156 140 132 124 239 157 54 106 172...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219222609775.jpg.chip.jpg</td>\n",
       "      <td>151 155 159 171 179 193 198 204 208 208 210 21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  ethnicity  gender                        img_name  \\\n",
       "0     1          2       0  20161219203650636.jpg.chip.jpg   \n",
       "1     1          2       0  20161219222752047.jpg.chip.jpg   \n",
       "2     1          2       0  20161219222832191.jpg.chip.jpg   \n",
       "3     1          2       0  20161220144911423.jpg.chip.jpg   \n",
       "4     1          2       0  20161220144914327.jpg.chip.jpg   \n",
       "5     1          2       0  20161220144957407.jpg.chip.jpg   \n",
       "6     1          2       0  20161220145040127.jpg.chip.jpg   \n",
       "7     1          2       0  20170109191125532.jpg.chip.jpg   \n",
       "8     1          2       0  20161219222749039.jpg.chip.jpg   \n",
       "9     1          2       0  20170109191209991.jpg.chip.jpg   \n",
       "10    1          2       0  20170109192102236.jpg.chip.jpg   \n",
       "11    1          2       0  20170109192222822.jpg.chip.jpg   \n",
       "12    1          2       0  20170109193535757.jpg.chip.jpg   \n",
       "13    1          2       0  20170110212743721.jpg.chip.jpg   \n",
       "14    1          2       0  20170110213009014.jpg.chip.jpg   \n",
       "15    1          2       0  20170110213415212.jpg.chip.jpg   \n",
       "16    1          2       0  20170110213523297.jpg.chip.jpg   \n",
       "17    1          2       0  20170109191453449.jpg.chip.jpg   \n",
       "18    1          2       0  20170116194202388.jpg.chip.jpg   \n",
       "19    1          2       0  20161219222609775.jpg.chip.jpg   \n",
       "\n",
       "                                               pixels  \n",
       "0   129 128 128 126 127 130 133 135 139 142 145 14...  \n",
       "1   164 74 111 168 169 171 175 182 184 188 193 199...  \n",
       "2   67 70 71 70 69 67 70 79 90 103 116 132 145 155...  \n",
       "3   193 197 198 200 199 200 202 203 204 205 208 21...  \n",
       "4   202 205 209 210 209 209 210 211 212 214 218 21...  \n",
       "5   195 198 200 200 198 198 199 199 198 197 197 19...  \n",
       "6   208 216 217 219 222 223 222 221 220 220 221 22...  \n",
       "7   99 142 169 177 179 181 183 186 187 186 191 190...  \n",
       "8   127 127 133 140 143 148 152 157 160 165 172 17...  \n",
       "9   199 211 211 214 216 216 219 221 222 224 219 21...  \n",
       "10  136 138 145 143 152 160 162 169 178 182 192 19...  \n",
       "11  253 253 253 253 252 251 250 209 228 250 221 19...  \n",
       "12  223 222 226 227 229 228 228 226 222 214 199 15...  \n",
       "13  181 185 186 185 182 180 180 177 183 177 175 17...  \n",
       "14  167 172 175 174 172 170 172 173 171 172 172 16...  \n",
       "15  99 122 138 147 148 149 149 150 153 157 162 166...  \n",
       "16  49 63 84 95 105 108 112 115 122 124 132 135 13...  \n",
       "17  226 225 223 166 136 167 170 173 176 177 180 18...  \n",
       "18  238 187 173 156 140 132 124 239 157 54 106 172...  \n",
       "19  151 155 159 171 179 193 198 204 208 208 210 21...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013e4aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23705 entries, 0 to 23704\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   age        23705 non-null  int64 \n",
      " 1   ethnicity  23705 non-null  int64 \n",
      " 2   gender     23705 non-null  int64 \n",
      " 3   img_name   23705 non-null  object\n",
      " 4   pixels     23705 non-null  object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 926.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047f12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))\n",
    "inputs = np.array(data['pixels'].tolist())\n",
    "inputs = inputs.reshape(inputs.shape[0],48,48,1)\n",
    "targets = data['gender']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.20, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b491ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a72a157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 48, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 24, 24, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 12, 12, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 256)         295168    \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 6, 6, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 3, 3, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 3, 3, 512)         1180160   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 1, 1, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,338,402\n",
      "Trainable params: 8,337,442\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(48,48,1)),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    \n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),    \n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation = 'relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "print(model.summary())\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "filepath1=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filepath2 = \"best_weights.hdf5\"\n",
    "checkpoint1 = ModelCheckpoint(filepath1, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint2 = ModelCheckpoint(filepath2, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint1,checkpoint2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3401dfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1414 - accuracy: 0.9427\n",
      "Epoch 00001: val_accuracy did not improve from 0.89141\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89141\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.1413 - accuracy: 0.9426 - val_loss: 0.2992 - val_accuracy: 0.8772\n",
      "Epoch 2/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9482\n",
      "Epoch 00002: val_accuracy did not improve from 0.89141\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.89141\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.1305 - accuracy: 0.9483 - val_loss: 0.3007 - val_accuracy: 0.8877\n",
      "Epoch 3/30\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9534\n",
      "Epoch 00003: val_accuracy did not improve from 0.89141\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.89141\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.1198 - accuracy: 0.9534 - val_loss: 0.3319 - val_accuracy: 0.8872\n",
      "Epoch 4/30\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9565\n",
      "Epoch 00004: val_accuracy did not improve from 0.89141\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.89141\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.1096 - accuracy: 0.9565 - val_loss: 0.3634 - val_accuracy: 0.8909\n",
      "Epoch 5/30\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9614\n",
      "Epoch 00005: val_accuracy did not improve from 0.89141\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.89141\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.1024 - accuracy: 0.9614 - val_loss: 0.2955 - val_accuracy: 0.8872\n",
      "Epoch 6/30\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.0942 - accuracy: 0.9642\n",
      "Epoch 00006: val_accuracy did not improve from 0.89141\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.89141\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0942 - accuracy: 0.9642 - val_loss: 0.3881 - val_accuracy: 0.8856\n",
      "Epoch 7/30\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.0916 - accuracy: 0.9662\n",
      "Epoch 00007: val_accuracy improved from 0.89141 to 0.89615, saving model to weights-improvement-07-0.90.hdf5\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.89141 to 0.89615, saving model to best_weights.hdf5\n",
      "1067/1067 [==============================] - 17s 16ms/step - loss: 0.0916 - accuracy: 0.9663 - val_loss: 0.3096 - val_accuracy: 0.8962\n",
      "Epoch 8/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9676\n",
      "Epoch 00008: val_accuracy did not improve from 0.89615\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.89615\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0842 - accuracy: 0.9677 - val_loss: 0.4150 - val_accuracy: 0.8919\n",
      "Epoch 9/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9695\n",
      "Epoch 00009: val_accuracy improved from 0.89615 to 0.90406, saving model to weights-improvement-09-0.90.hdf5\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.89615 to 0.90406, saving model to best_weights.hdf5\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0792 - accuracy: 0.9694 - val_loss: 0.3487 - val_accuracy: 0.9041\n",
      "Epoch 10/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9721\n",
      "Epoch 00010: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0711 - accuracy: 0.9722 - val_loss: 0.3953 - val_accuracy: 0.8935\n",
      "Epoch 11/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9722\n",
      "Epoch 00011: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0738 - accuracy: 0.9721 - val_loss: 0.4244 - val_accuracy: 0.8914\n",
      "Epoch 12/30\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9761\n",
      "Epoch 00012: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0646 - accuracy: 0.9762 - val_loss: 0.4944 - val_accuracy: 0.8888\n",
      "Epoch 13/30\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9773\n",
      "Epoch 00013: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0628 - accuracy: 0.9773 - val_loss: 0.4587 - val_accuracy: 0.8998\n",
      "Epoch 14/30\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.0550 - accuracy: 0.9799\n",
      "Epoch 00014: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0549 - accuracy: 0.9800 - val_loss: 0.4181 - val_accuracy: 0.8914\n",
      "Epoch 15/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0564 - accuracy: 0.9788\n",
      "Epoch 00015: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0564 - accuracy: 0.9788 - val_loss: 0.4072 - val_accuracy: 0.8940\n",
      "Epoch 16/30\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.0509 - accuracy: 0.9802\n",
      "Epoch 00016: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0510 - accuracy: 0.9802 - val_loss: 0.3577 - val_accuracy: 0.8798\n",
      "Epoch 17/30\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9826\n",
      "Epoch 00017: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0495 - accuracy: 0.9826 - val_loss: 0.3484 - val_accuracy: 0.8962\n",
      "Epoch 18/30\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.0434 - accuracy: 0.9844\n",
      "Epoch 00018: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0434 - accuracy: 0.9845 - val_loss: 0.5015 - val_accuracy: 0.8956\n",
      "Epoch 19/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0465 - accuracy: 0.9836\n",
      "Epoch 00019: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0464 - accuracy: 0.9837 - val_loss: 0.4886 - val_accuracy: 0.8956\n",
      "Epoch 20/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0467 - accuracy: 0.9836\n",
      "Epoch 00020: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0467 - accuracy: 0.9835 - val_loss: 0.4721 - val_accuracy: 0.8967\n",
      "Epoch 21/30\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9845\n",
      "Epoch 00021: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0412 - accuracy: 0.9845 - val_loss: 0.4486 - val_accuracy: 0.8909\n",
      "Epoch 22/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0405 - accuracy: 0.9853\n",
      "Epoch 00022: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0407 - accuracy: 0.9853 - val_loss: 0.7112 - val_accuracy: 0.8962\n",
      "Epoch 23/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0393 - accuracy: 0.9860\n",
      "Epoch 00023: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0394 - accuracy: 0.9859 - val_loss: 0.4038 - val_accuracy: 0.8867\n",
      "Epoch 24/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0413 - accuracy: 0.9859\n",
      "Epoch 00024: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0412 - accuracy: 0.9859 - val_loss: 0.4141 - val_accuracy: 0.8967\n",
      "Epoch 25/30\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9889\n",
      "Epoch 00025: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0335 - accuracy: 0.9889 - val_loss: 0.4172 - val_accuracy: 0.8935\n",
      "Epoch 26/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0361 - accuracy: 0.9870\n",
      "Epoch 00026: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0363 - accuracy: 0.9870 - val_loss: 0.4644 - val_accuracy: 0.8967\n",
      "Epoch 27/30\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.0348 - accuracy: 0.9881\n",
      "Epoch 00027: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0347 - accuracy: 0.9881 - val_loss: 0.4643 - val_accuracy: 0.8988\n",
      "Epoch 28/30\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.0324 - accuracy: 0.9890\n",
      "Epoch 00028: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0324 - accuracy: 0.9890 - val_loss: 0.4247 - val_accuracy: 0.8962\n",
      "Epoch 29/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9880\n",
      "Epoch 00029: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0337 - accuracy: 0.9879 - val_loss: 0.4203 - val_accuracy: 0.8956\n",
      "Epoch 30/30\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.0318 - accuracy: 0.9897\n",
      "Epoch 00030: val_accuracy did not improve from 0.90406\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.90406\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.0317 - accuracy: 0.9897 - val_loss: 0.3579 - val_accuracy: 0.9004\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=16, validation_split=0.1, epochs = 30, verbose=1, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "738c82a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/149 [==============================] - 2s 7ms/step - loss: 0.4067 - accuracy: 0.8962\n",
      "loss:0.4066978693008423 \n",
      " acc:0.8962244391441345\n"
     ]
    }
   ],
   "source": [
    "list1 = model.evaluate(X_test, y_test)\n",
    "print(f'loss:{list1[0]} \\n acc:{list1[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc02048",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(48,48,1)),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    \n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),    \n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation = 'relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "print(model.summary())\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "filepath1=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "filepath2 = \"best_weights.hdf5\"\n",
    "checkpoint1 = ModelCheckpoint(filepath1, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint2 = ModelCheckpoint(filepath2, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint1,checkpoint2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
